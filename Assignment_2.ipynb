{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, cost: 326978321.06\n",
      "Iter: 1, cost: 302058045.33\n",
      "Iter: 2, cost: 277306988.12\n",
      "Iter: 3, cost: 247761898.67\n",
      "Iter: 4, cost: 207824461.04\n",
      "Iter: 5, cost: 156870607.87\n",
      "Iter: 6, cost: 110003234.79\n",
      "Iter: 7, cost: 86188253.52\n",
      "Iter: 8, cost: 77226805.34\n",
      "Iter: 9, cost: 71412900.89\n",
      "Iter: 10, cost: 66592336.14\n",
      "Iter: 199, cost: 2241282.01\n",
      "Iter: 0, cost: 326413225.26\n",
      "Iter: 1, cost: 301870142.76\n",
      "Iter: 2, cost: 277593537.22\n",
      "Iter: 3, cost: 248750621.46\n",
      "Iter: 4, cost: 209728707.05\n",
      "Iter: 5, cost: 159319275.27\n",
      "Iter: 6, cost: 111625287.24\n",
      "Iter: 7, cost: 86514161.70\n",
      "Iter: 8, cost: 77221142.73\n",
      "Iter: 9, cost: 71378396.86\n",
      "Iter: 10, cost: 66559917.22\n",
      "Iter: 199, cost: 2234355.61\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mvfile = 'movies.csv'\n",
    "rating_file = 'movieratings.csv'\n",
    "\n",
    "class Embeddings:\n",
    "\n",
    "    def __init__(emb, mvfile, rating_file):\n",
    "        emb.movies = emb.load_movies(mvfile)\n",
    "        emb.movie_ratings = emb.load_ratings(rating_file)\n",
    "        emb.cooccurences = emb.make_cooccurence()\n",
    "        #emb.save_cooccurences(\"cooccurrence.csv.gz\")\n",
    "        emb.embeddings = emb.train()\n",
    "        emb.embeddings = emb.train()\n",
    "\n",
    "    def load_movies(emb, mvfile, delimiter=\"|\"):\n",
    "        movies = {}\n",
    "        movie_dict = {}\n",
    "        with open(mvfile) as f:\n",
    "            for line in f:\n",
    "                x = line.split(delimiter)\n",
    "                movies[int(x[0])] = x[1]\n",
    "        return movies\n",
    "\n",
    "    def load_ratings(emb, rating_file, delimiter=\",\"):\n",
    "        users_dict = {}\n",
    "        with open(rating_file) as f:\n",
    "            for line in f:\n",
    "                x = line.split(delimiter)\n",
    "                movie_id, user_id, rating = (int(x[0]), int(x[1]), int(x[2]))\n",
    "                if not rating:\n",
    "                    continue\n",
    "                if user_id in users_dict:\n",
    "                    users_dict[user_id].add(movie_id)\n",
    "                else:\n",
    "                    users_dict[user_id] = set([movie_id])\n",
    "        return users_dict\n",
    "\n",
    "    def make_cooccurence(emb):\n",
    "        movies = len(emb.movies.keys())\n",
    "        cooccurences = np.zeros((movies, movies))\n",
    "        # the u's are dictionaries every user's movie ratings\n",
    "        for u in emb.movie_ratings.values():\n",
    "            for i in u:\n",
    "                for j in u:\n",
    "                    cooccurences[i-1,j-1] += 1\n",
    "        return cooccurences\n",
    "\n",
    "    def save_cooccurences(emb, file):\n",
    "        np.savetxt(file, emb.cooccurences)\n",
    "        return\n",
    "   \n",
    "    def train(emb):\n",
    "        n = emb.cooccurences.shape[0]\n",
    "        k = 300\n",
    "        l = 0.00001\n",
    "        iterations = 200\n",
    "        v =  0.657304632 * np.random.randn(n, k) # ----- 2 (b) 0.657304632 \n",
    "        #v = np.zeros((n, k))  # -------- 2 (a)\n",
    "        for i in range(iterations):\n",
    "            if i < 11 or i == iterations-1:\n",
    "                print \"Iter: %d, cost: %.2f\" % (i, emb.cost(v))\n",
    "            v = emb.gradient(v, l)\n",
    "        return v\n",
    "    \n",
    "\n",
    "    def gradient(emb, v, l):\n",
    "        grad = np.dot(v, v.T) - emb.cooccurences\n",
    "        np.fill_diagonal(grad, 0)\n",
    "        return v-4*l*np.dot(grad, v)\n",
    "\n",
    "    def cost(emb, v):\n",
    "        c = np.square(emb.cooccurences - np.dot(v,v.T))\n",
    "        np.fill_diagonal(c, 0)\n",
    "        return np.sum(c)\n",
    "\n",
    "    def cosine_similarity(emb, v1, v2):\n",
    "        a = np.linalg.norm(v1)\n",
    "        b = np.linalg.norm(v2)\n",
    "        return np.divide(np.dot(v1,v2), a*b)\n",
    "\n",
    "    def recommend1(emb, movie, r):\n",
    "        if movie not in emb.movies:\n",
    "            return []\n",
    "\n",
    "        scores = []\n",
    "        v = emb.embeddings[movie-1,:]\n",
    "\n",
    "        for i, name in emb.movies.items():\n",
    "            s = emb.cosine_similarity(v, emb.embeddings[i-1,:])\n",
    "            scores.append((name, s))\n",
    "\n",
    "        return sorted(scores, key=lambda x: x[1], reverse=True)[:r]\n",
    "\n",
    "    def recommend2(emb, movies, r):\n",
    "        n = 0\n",
    "        v = np.zeros(emb.embeddings.shape[1])\n",
    "        for m in movies:\n",
    "            if m not in emb.movies:\n",
    "                continue\n",
    "\n",
    "            v += emb.embeddings[m-1,:]\n",
    "            n += 1\n",
    "\n",
    "        v = np.divide(v, n)\n",
    "        scores = []\n",
    "\n",
    "        for i, name in emb.movies.items():\n",
    "            s = emb.cosine_similarity(v, emb.embeddings[i-1,:])\n",
    "            scores.append((name, s))\n",
    "\n",
    "        return sorted(scores, key=lambda x: x[1], reverse=True)[:r]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    me = Embeddings(\"movies.csv\", \"movieratings.csv\")\n",
    "\n",
    "    recommend1 = me.recommend1(95, 10)\n",
    "    recommend2 = me.recommend2([1, 94], 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Aladdin (1992)\\r\\n', 1.0),\n",
       " ('Beauty and the Beast (1991)\\r\\n', 0.81897239160501878),\n",
       " ('Lion King, The (1994)\\r\\n', 0.79930836950035722),\n",
       " ('Back to the Future (1985)\\r\\n', 0.76161930093299768),\n",
       " ('Jurassic Park (1993)\\r\\n', 0.76113137158235633),\n",
       " ('Apollo 13 (1995)\\r\\n', 0.74140970993223632),\n",
       " ('Toy Story (1995)\\r\\n', 0.72923166996065103),\n",
       " ('Groundhog Day (1993)\\r\\n', 0.72823190991076003),\n",
       " ('Empire Strikes Back, The (1980)\\r\\n', 0.72362418667676431),\n",
       " ('Forrest Gump (1994)\\r\\n', 0.71336650049064743)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Toy Story (1995)\\r\\n', 0.96016816450032139),\n",
       " ('Star Wars (1977)\\r\\n', 0.83726859206004123),\n",
       " ('Return of the Jedi (1983)\\r\\n', 0.83462954331749939),\n",
       " ('Apollo 13 (1995)\\r\\n', 0.79237546908290224),\n",
       " ('Independence Day (ID4) (1996)\\r\\n', 0.78338197246002272),\n",
       " ('Back to the Future (1985)\\r\\n', 0.77820603187067328),\n",
       " ('Men in Black (1997)\\r\\n', 0.77521234241974779),\n",
       " ('Star Trek: First Contact (1996)\\r\\n', 0.77361270009993488),\n",
       " ('Raiders of the Lost Ark (1981)\\r\\n', 0.7733392717131421),\n",
       " ('Empire Strikes Back, The (1980)\\r\\n', 0.77286425123663283)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Question - 2 IDS 576 Assignment 2\n",
    "# Implementing a Twitter Bot\n",
    "#%pwd\n",
    "#%cd\n",
    "\n",
    "import tweepy\n",
    "from time import sleep\n",
    "\n",
    "#reading credentials for app directly from stored credential file\n",
    "%run '/Users/adityabhandari/Desktop/Spring 2018/IDS 576 - ADV PRED/Assignment/Assignment-2/twitterbot_credentials.py'\n",
    "\n",
    "# Create variables for each key, secret, token\n",
    "#consumer_key = 'abcdef'\n",
    "#consumer_secret = 'abcdef'\n",
    "#access_token = 'abcdef'\n",
    "#access_token_secret = 'abcdef'\n",
    "\n",
    "# Creating an OAuthHandler instance into which weâ€™ll pass our consumer token and secretand integrate with API. \n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Write a tweet to push to our Twitter account\n",
    "tweet = 'Hello, world!'\n",
    "api.update_status(status=tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open text file text_corpus.txt \n",
    "my_file = open('text_corpus/text_corpus.txt', 'r')\n",
    "\n",
    "# Read lines one by one from my_file and assign to file_lines variable\n",
    "file_lines = my_file.readlines()\n",
    "\n",
    "# Close file\n",
    "my_file.close()\n",
    "\n",
    "# Create a for loop to iterate over file_lines\n",
    "for line in file_lines:\n",
    "    try:\n",
    "        print(line)\n",
    "\n",
    "    # Add if statement to ensure that blank lines are skipped\n",
    "        if line != '\\n':\n",
    "            api.update_status(line)\n",
    "\n",
    "    # Add an else statement with pass to conclude the conditional statement\n",
    "        else:\n",
    "            pass\n",
    "    except:tweepy.TweepError as e:\n",
    "            print(e.reason)\n",
    "        \n",
    "# Add sleep method to space tweets by 5 seconds each\n",
    "    sleep(360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print('Training model...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=3,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Small LSTM Network to Generate Text alternative approach\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"/Users/adityabhandari/Desktop/Spring 2018/IDS 576 - ADV PRED/Assignment/Assignment-2/text_corpus/text_corpus.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=2, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-19-1.9435.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bonus question N-gram repeated steps for question2\n",
    "import math\n",
    "import re\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def tokenize(input_file, encoding):\n",
    "    lst =[]\n",
    "    with open(input_file, 'r', encoding=encoding) as f:\n",
    "        for sent in f:\n",
    "            sent = sent.lower()\n",
    "            sent = re.sub(\"[A-z0-9\\'\\\"`\\|\\/\\+\\#\\,\\)\\(\\?\\!\\-\\:\\=\\;\\.\\Â«\\Â»\\â€”\\@]\", '', sent)\n",
    "            sent = re.findall('\\w+', sent)\n",
    "            for word in sent:\n",
    "                lst.append(word)\n",
    "    return lst\n",
    "\n",
    "\n",
    "def ngrams_split(lst, n):\n",
    "    counts = dict()\n",
    "    grams = [' '.join(lst[i:i+n]) for i in range(len(lst)-n)]\n",
    "    for gram in grams:\n",
    "        if gram not in counts:\n",
    "            counts[gram] = 1\n",
    "        else:\n",
    "            counts[gram] += 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def list_add(counts):\n",
    "    ngrams = []\n",
    "    for key, val in counts.items():\n",
    "        ngrams.append((val, key))\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def gram_add(lst, n):\n",
    "    ng = []\n",
    "    grams = [' '.join(lst[i:i+n]) for i in range(len(lst)-n)]\n",
    "    for gram in grams:\n",
    "        ng.append(gram)\n",
    "    return ng\n",
    "\n",
    "\n",
    "def two_gram_count(input_file, encoding, n_filter, n):\n",
    "    output_file = []\n",
    "    lst = tokenize(input_file, encoding) #tokenize\n",
    "    n_words = len(lst)\n",
    "    counts = ngrams_split(lst, n) #spliting into ngrams\n",
    "    ngrams = list_add(counts)  #ading ngrmas to list\n",
    "    for key, val in ngrams:\n",
    "        if int(key) >= n_filter:\n",
    "            ngram_freq = math.log(key/n_words)\n",
    "            num = key*n_words\n",
    "            f1 = lst.count(val.split()[0])\n",
    "            f2 = lst.count(val.split()[1])\n",
    "            mi = math.pow(math.log(num/(f1*f2), 10), 2)\n",
    "            ngram_prob = math.log(key/f1, 10)\n",
    "            output_file.append((ngram_freq, mi, ngram_prob, key, val))\n",
    "    return output_file\n",
    "\n",
    "\n",
    "def three_gram_count(input_file, encoding, n_filter, n):\n",
    "    output_file = []\n",
    "    lst = tokenize(input_file, encoding) #tokenize\n",
    "    n_words = len(lst)\n",
    "    counts = ngrams_split(lst, n) #spliting into ngrams\n",
    "    ngrams = list_add(counts)  #ading ngrmas to list\n",
    "    ng = gram_add(lst, 2)\n",
    "    for key, val in ngrams:\n",
    "        if int(key) >= n_filter:\n",
    "            ngram_freq = math.log(key/n_words, 10)\n",
    "            num = key*n_words\n",
    "            c2gram = ng.count(val.split()[0] + \" \" + val.split()[1])\n",
    "            f1 = lst.count(val.split()[0])\n",
    "            f2 = lst.count(val.split()[1])\n",
    "            f3 = lst.count(val.split()[2])\n",
    "            mi = math.pow(math.log(num/(f1*f2*f3), 10), 2)\n",
    "            ngram_prob = math.log(key/c2gram, 10)\n",
    "            output_file.append((ngram_freq, mi, ngram_prob, key, val))\n",
    "    return output_file\n",
    "\n",
    "\n",
    "def four_grams_count(input_file, encoding, n_filter, n):\n",
    "    output_file = []\n",
    "    lst = tokenize(input_file, encoding) #tokenize\n",
    "    n_words = len(lst)\n",
    "    counts = ngrams_split(lst, n) #spliting into ngrams\n",
    "    ngrams = list_add(counts)  #ading ngrmas to list\n",
    "    ng2 = gram_add(lst, 2)\n",
    "    for key, val in ngrams:\n",
    "        if int(key) >= n_filter:\n",
    "            ngram_freq = math.log(key/n_words, 10)\n",
    "            num = key*n_words\n",
    "            c1gram = ng2.count(val.split()[0] + \" \" + val.split()[1])\n",
    "            c2gram = ng2.count(val.split()[1] + \" \" + val.split()[2])\n",
    "            c3gram = ng2.count(val.split()[2] + \" \" + val.split()[3])\n",
    "            f1 = lst.count(val.split()[0])\n",
    "            f2 = lst.count(val.split()[1])\n",
    "            f3 = lst.count(val.split()[2])\n",
    "            f4 = lst.count(val.split()[3])\n",
    "            mi = math.pow(math.log(num/(f1*f2*f3*f4), 10), 2)\n",
    "            prob1 = c1gram/f1\n",
    "            prob2 = c2gram/f2\n",
    "            prob3 = c3gram/f3\n",
    "            ngram_prob = math.log(prob1, 10) + math.log(prob2, 10) +    math.log(prob3, 10)\n",
    "            output_file.append((ngram_freq, mi, ngram_prob, key, val))\n",
    "    return output_file\n",
    "\n",
    "\n",
    "def n_grams_stat(input_file, encoding, n_filter, n):\n",
    "    output_file = []\n",
    "    if n == 2:\n",
    "        for i in two_gram_count(input_file, encoding, n_filter, n):\n",
    "            output_file.append(i)\n",
    "    elif n == 3:\n",
    "        for i in three_gram_count(input_file, encoding, n_filter, n):\n",
    "            output_file.append(i)\n",
    "    elif n == 4:\n",
    "        for i in four_grams_count(input_file, encoding, n_filter, n):\n",
    "            output_file.append(i)\n",
    "    return output_file\n",
    "\n",
    "start_time = datetime.now()\n",
    "for a, b, c in n_grams_stat(\"C:/Users/bhandari/Downloads/Adv. Predictive/Assignment 2/text_corpus.txt\",'utf-8', n_filter=3, n=4):\n",
    "    print(a, b, c)\n",
    "    with open(\"C:/Users/bhandari/Downloads/Adv. Predictive/Assignment 2/men_4grams.csv\", 'dwwaa') as f:\n",
    "        f.write(\"Aditya Bhandari\")\n",
    "        f.write(str(a)  +\", \"+ str(b) + \", \"+ str(c) + '\\n ')\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
